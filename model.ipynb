{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a15e64",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-19T12:51:44.005745Z",
     "iopub.status.busy": "2024-10-19T12:51:44.005231Z",
     "iopub.status.idle": "2024-10-19T12:51:45.259734Z",
     "shell.execute_reply": "2024-10-19T12:51:45.258074Z"
    },
    "papermill": {
     "duration": 1.263814,
     "end_time": "2024-10-19T12:51:45.262728",
     "exception": false,
     "start_time": "2024-10-19T12:51:43.998914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/average-time-spent-by-a-user-on-social-media/dummy_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d9eacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T12:51:45.273276Z",
     "iopub.status.busy": "2024-10-19T12:51:45.272729Z",
     "iopub.status.idle": "2024-10-19T12:51:45.317869Z",
     "shell.execute_reply": "2024-10-19T12:51:45.316336Z"
    },
    "papermill": {
     "duration": 0.053479,
     "end_time": "2024-10-19T12:51:45.320694",
     "exception": false,
     "start_time": "2024-10-19T12:51:45.267215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age      gender  time_spent   platform  interests        location  \\\n",
      "0   56        male           3  Instagram     Sports  United Kingdom   \n",
      "1   46      female           2   Facebook     Travel  United Kingdom   \n",
      "2   32        male           8  Instagram     Sports       Australia   \n",
      "3   60  non-binary           5  Instagram     Travel  United Kingdom   \n",
      "4   25        male           1  Instagram  Lifestlye       Australia   \n",
      "\n",
      "  demographics         profession  income  indebt  isHomeOwner  Owns_Car  \n",
      "0        Urban  Software Engineer   19774    True        False     False  \n",
      "1        Urban            Student   10564    True         True      True  \n",
      "2    Sub_Urban   Marketer Manager   13258   False        False     False  \n",
      "3        Urban            Student   12500   False         True     False  \n",
      "4        Urban  Software Engineer   14566   False         True      True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with the actual file path of your CSV file printed in the previous code\n",
    "file_path = '/kaggle/input/average-time-spent-by-a-user-on-social-media/dummy_data.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "dp = pd.read_csv(file_path)\n",
    "\n",
    "# View the first 5 rows of the DataFrame\n",
    "print(dp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0768abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T12:51:45.331050Z",
     "iopub.status.busy": "2024-10-19T12:51:45.330491Z",
     "iopub.status.idle": "2024-10-19T12:51:47.841015Z",
     "shell.execute_reply": "2024-10-19T12:51:47.839508Z"
    },
    "papermill": {
     "duration": 2.519607,
     "end_time": "2024-10-19T12:51:47.844591",
     "exception": false,
     "start_time": "2024-10-19T12:51:45.324984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create severity levels (0-3) based on time spent quartiles in ascending order\n",
    "dp['severity'] = pd.qcut(dp['time_spent'], q=4, labels=[0, 1, 2, 3], retbins=True)[0]\n",
    "\n",
    "# Feature Engineering\n",
    "# Create interaction features (Example: 'age' * 'income')\n",
    "dp['age_income_interaction'] = dp['age'] * dp['income']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['gender', 'platform', 'location']\n",
    "numerical_features = ['age', 'time_spent', 'income', 'age_income_interaction']\n",
    "\n",
    "# Handle missing values and standardize numerical variables\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prepare the features and target variable\n",
    "X = dp.drop(columns='severity')\n",
    "y = dp['severity']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97221e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T12:51:47.856978Z",
     "iopub.status.busy": "2024-10-19T12:51:47.856230Z",
     "iopub.status.idle": "2024-10-19T12:52:07.056949Z",
     "shell.execute_reply": "2024-10-19T12:52:07.055397Z"
    },
    "papermill": {
     "duration": 19.21022,
     "end_time": "2024-10-19T12:52:07.059860",
     "exception": false,
     "start_time": "2024-10-19T12:51:47.849640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Training Accuracy: 1.00\n",
      "Validation Accuracy: 1.00\n",
      "Testing Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Create Logistic Regression and Random Forest models\n",
    "lr_model = LogisticRegression(max_iter=200)\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters for optimization (for XGBoost as an example)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning on XGBoost\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Best model from XGBoost\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(f\"Best XGBoost hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Define the soft voting classifier combining XGBoost, Logistic Regression, and Random Forest\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_model), \n",
    "    ('lr', lr_model), \n",
    "    ('rf', rf_model)], \n",
    "    voting='soft')  # soft voting averages the predicted probabilities\n",
    "\n",
    "# Train the soft voting classifier\n",
    "voting_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_predictions = voting_clf.predict(X_train_processed)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "\n",
    "# Optionally, if you have a validation set, evaluate on it\n",
    "# Split the training set into training and validation for further evaluation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the voting classifier on the new training set\n",
    "voting_clf.fit(preprocessor.fit_transform(X_train_final), y_train_final)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = voting_clf.predict(preprocessor.transform(X_val))\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = voting_clf.predict(X_test_processed)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051f3cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T12:52:07.071342Z",
     "iopub.status.busy": "2024-10-19T12:52:07.070825Z",
     "iopub.status.idle": "2024-10-19T12:52:07.561974Z",
     "shell.execute_reply": "2024-10-19T12:52:07.560076Z"
    },
    "papermill": {
     "duration": 0.500621,
     "end_time": "2024-10-19T12:52:07.565175",
     "exception": false,
     "start_time": "2024-10-19T12:52:07.064554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.00\n",
      "Predictions: ['Moderate' 'Critical' 'Low' 'Critical' 'Low' 'Low' 'Low' 'Low' 'Critical'\n",
      " 'Critical' 'High' 'Critical' 'Low' 'Low' 'High' 'Moderate' 'Low' 'Low'\n",
      " 'Critical' 'Moderate' 'Critical' 'High' 'Low' 'Moderate' 'High' 'Low'\n",
      " 'High' 'Low' 'Moderate' 'Moderate' 'Low' 'Low' 'Critical' 'High' 'Low'\n",
      " 'High' 'Critical' 'High' 'Critical' 'Moderate' 'Low' 'Moderate'\n",
      " 'Moderate' 'Moderate' 'Low' 'Critical' 'Low' 'Low' 'High' 'Critical'\n",
      " 'Moderate' 'High' 'High' 'Moderate' 'Low' 'Low' 'Moderate' 'High' 'Low'\n",
      " 'Moderate' 'High' 'High' 'Critical' 'Critical' 'Critical' 'Moderate'\n",
      " 'High' 'Low' 'Low' 'Critical' 'Low' 'Low' 'Moderate' 'High' 'High' 'High'\n",
      " 'Moderate' 'Moderate' 'Moderate' 'High' 'Moderate' 'Moderate' 'Low'\n",
      " 'Moderate' 'Moderate' 'Moderate' 'Moderate' 'Low' 'Moderate' 'High'\n",
      " 'Moderate' 'High' 'Low' 'Critical' 'High' 'Moderate' 'Critical' 'High'\n",
      " 'Low' 'Critical' 'Moderate' 'Low' 'Critical' 'Low' 'Critical' 'Moderate'\n",
      " 'Low' 'Critical' 'Moderate' 'Low' 'High' 'Moderate' 'Low' 'Low'\n",
      " 'Moderate' 'Moderate' 'Low' 'Critical' 'Critical' 'High' 'Critical'\n",
      " 'High' 'Moderate' 'Low' 'Low' 'Critical' 'High' 'High' 'Low' 'High' 'Low'\n",
      " 'Low' 'High' 'Critical' 'Critical' 'Moderate' 'Critical' 'Critical'\n",
      " 'Critical' 'Moderate' 'High' 'High' 'High' 'Low' 'Low' 'Low' 'Low' 'High'\n",
      " 'Moderate' 'Critical' 'Low' 'Critical' 'Critical' 'High' 'Moderate'\n",
      " 'Moderate' 'High' 'Moderate' 'Moderate' 'High' 'High' 'Moderate' 'High'\n",
      " 'Moderate' 'High' 'Low' 'Low' 'Low' 'High' 'Low' 'Low' 'Moderate'\n",
      " 'Moderate' 'High' 'Low' 'Critical' 'Moderate' 'Critical' 'Low' 'Moderate'\n",
      " 'Critical' 'Low' 'Critical' 'Moderate' 'Critical' 'High' 'High' 'Low'\n",
      " 'Low' 'Moderate' 'Critical' 'Moderate' 'High' 'Critical' 'Moderate' 'Low'\n",
      " 'Moderate' 'Low' 'Critical' 'Critical']\n",
      "Actual: ['Moderate', 'Critical', 'Low', 'Critical', 'Low', ..., 'Low', 'Moderate', 'Low', 'Critical', 'Critical']\n",
      "Length: 200\n",
      "Categories (4, object): ['Low' < 'Moderate' < 'High' < 'Critical']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Replace with actual file path\n",
    "file_path = '/kaggle/input/average-time-spent-by-a-user-on-social-media/dummy_data.csv'\n",
    "\n",
    "# Load data\n",
    "dp = pd.read_csv(file_path)\n",
    "\n",
    "# Define severity based on time spent using quantiles (Low, Moderate, High, Critical)\n",
    "dp['severity'] = pd.qcut(dp['time_spent'], q=4, labels=['Low', 'Moderate', 'High', 'Critical'])\n",
    "\n",
    "# Feature Engineering: Create interaction features (age * income)\n",
    "dp['age_income_interaction'] = dp['age'] * dp['income']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['gender', 'platform', 'location']\n",
    "numerical_features = ['age', 'time_spent', 'income', 'age_income_interaction']\n",
    "\n",
    "# Create pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prepare the features and target variable\n",
    "X = dp.drop(columns='severity')\n",
    "y = dp['severity']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Create models: XGBoost, Logistic Regression, Random Forest\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "lr_model = LogisticRegression(max_iter=200)\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Define the soft voting classifier combining XGBoost, Logistic Regression, and Random Forest\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', xgb_model), \n",
    "    ('lr', lr_model), \n",
    "    ('rf', rf_model)], \n",
    "    voting='soft')\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = voting_clf.predict(X_test_processed)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print predictions for test set\n",
    "print(\"Predictions:\", test_predictions)\n",
    "print(\"Actual:\", y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfd52c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T12:52:07.576396Z",
     "iopub.status.busy": "2024-10-19T12:52:07.575975Z",
     "iopub.status.idle": "2024-10-19T12:52:07.612403Z",
     "shell.execute_reply": "2024-10-19T12:52:07.611236Z"
    },
    "papermill": {
     "duration": 0.045361,
     "end_time": "2024-10-19T12:52:07.615268",
     "exception": false,
     "start_time": "2024-10-19T12:52:07.569907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input Data:\n",
      "   age  gender  time_spent   platform  location  income  \\\n",
      "0   18    Male           1  Instagram     Urban   30000   \n",
      "1   25  Female           5    Twitter  Suburban   50000   \n",
      "2   35  Female           7   Facebook     Rural   70000   \n",
      "3   45    Male          20   Snapchat     Urban   90000   \n",
      "\n",
      "   age_income_interaction  \n",
      "0                  540000  \n",
      "1                 1250000  \n",
      "2                 2450000  \n",
      "3                 4050000  \n",
      "Predicted Severity Levels: ['Low' 'Moderate' 'High' 'Critical']\n"
     ]
    }
   ],
   "source": [
    "# Sample inputs corresponding to severity levels: Low, Moderate, High, Critical\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': [18, 25, 35, 45],                 # Varied ages for diverse users\n",
    "    'gender': ['Male', 'Female', 'Female', 'Male'],   # Gender diversity\n",
    "    'time_spent': [1, 5, 7, 20],          # Time spent (Low, Moderate, High, Critical)\n",
    "    'platform': ['Instagram', 'Twitter', 'Facebook', 'Snapchat'],  # Different social media platforms\n",
    "    'location': ['Urban', 'Suburban', 'Rural', 'Urban'],           # Varied locations\n",
    "    'income': [30000, 50000, 70000, 90000]   # Varied income levels for interaction feature\n",
    "})\n",
    "\n",
    "# Feature Engineering: Create interaction features (age * income)\n",
    "sample_data['age_income_interaction'] = sample_data['age'] * sample_data['income']\n",
    "\n",
    "# Preprocess the sample data\n",
    "sample_data_processed = preprocessor.transform(sample_data)\n",
    "\n",
    "# Predict the severity using the trained model\n",
    "sample_predictions = voting_clf.predict(sample_data_processed)\n",
    "\n",
    "# Print the results\n",
    "print(\"Sample Input Data:\")\n",
    "print(sample_data)\n",
    "print(\"Predicted Severity Levels:\", sample_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581dc929",
   "metadata": {
    "papermill": {
     "duration": 0.00439,
     "end_time": "2024-10-19T12:52:07.624339",
     "exception": false,
     "start_time": "2024-10-19T12:52:07.619949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4321640,
     "sourceId": 7426997,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.804056,
   "end_time": "2024-10-19T12:52:08.352075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-19T12:51:40.548019",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
